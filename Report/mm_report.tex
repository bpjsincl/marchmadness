%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}

% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


\usepackage[cmex10]{amsmath}

% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/




%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/

% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.


%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.


\usepackage{float}


% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{longtable}
\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Predicting NCAA March Madness (Brackets, Game Outcomes) (with MI Techniques)}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{
\IEEEauthorblockN{Brian Sinclair}
\IEEEauthorblockA{Systems Desing Engineering\\
University of Waterloo\\
Waterloo, Canada\\
Email: bpjsincl@uwaterloo.ca}
\and
\IEEEauthorblockN{Matthew Chong}
\IEEEauthorblockA{Systems Desing Engineering\\
University of Waterloo\\
Waterloo, Canada\\
Email: mt2chong@uwaterloo.ca}
\and
\IEEEauthorblockN{D. Scott Neil}
\IEEEauthorblockA{Systems Desing Engineering\\
University of Waterloo\\
Waterloo, Canada\\
Email: dsneil@uwaterloo.ca}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% make the title area
\maketitle


\begin{abstract}
%Mention something about kaggle
%change that second scentence wording

Bracket and Game prediction is continually growing in popularity due to revenue production over various gambling mediums.
March Madness is a sporting event held every year, with competing Division I NCAA college basketball teams, and attracts significant gambling and competition to predict winners.
Outside of the typical gambling realm, Kaggle has held a competition this season, offering \$15,000 to the winner.
The researchers did not participate, but are using this competition, its data and results, as a foundation for the analysis.
Many tactics are currently being reasearched and employed, such as: ANNs, varying regression techniques, probabalistic optimization, SVM's, and genetic algorithms.
However, due to high uncertainty in outcomes, none have been extremely accurate.
The researchers will apply Artificial Neural Networks and Random Forests, with data gathered from the Kaggle Competition and Ken Pomeroy.
Using these techniques, the researchers will attempt to predict the winners of this tournament, constructing a traditional bracket and for each potential matchup of teams, the and will do so through calculating the probability of that outcome occurring for each round of play.
The previous seasons will assist in the process of training and testing, and models will be benchmarked against specific seasons.
Additionally, to determine technique's success, the researchers will use Logarithmic Loss and compare predictions against results from Kaggle.
\end{abstract}


\section{Introduction}
March Madness is a 64 team, single-knockout tournament <finish this scentence explaining MM>.
Betting on this tournament is extremely popular and a total of \$12 billion was spent on gambling related bets in 2012 \cite{BradTuttle-2012}. 
The most popular bet made comes in the form of the March Madness bracket. So popular that more Americans filled out a bracket in 2012 than voted in the Presidential Election \cite{DavidHolmes-2013}. 
Predicting this bracket includes choosing the winner of each game in the tournament for each of the six rounds. 
The difficulty of predicting a perfect March Madness bracket is widely known and accepted and Warren Buffet has even offered a \$1 billion cash prize to anyone who is successful \cite{wbuff_bill}.

Attempts to perfectly predict March Madness results using Machine Learning techniques has seen a recent surge in popularity. 
Hand calculated predictions used to be the norm, but more advanced MI techniques are starting to be used with the wealth of data now available online. 
However, there is no greed upon or industry standardized method for predicting a bracket, or even the outcome of a basketball game. 
Many tactics are used, such as: neural nets \cite{KarlLeswing}, varying regression and probabalistic optimization techniques \cite{adams-dahl-murray-2010a, DannyTarlow, BradyWest, BradleyCarlin, ProbModelsNCAA-1991, MoreProbModelsNCAA-1996}, support vector machines \cite{KennethDeakins}, and genetic algorithms \cite{ScottTurner}, but due to high uncertainty in outcomes, none have been extremely accurate, and continuous improvements are being made.

Kaggle, a machine-learning competition repository, held a competition this year for predicting the outcome of each possible matchup in the March Madness tournament (\url{KAAGLE}).
The researchers did not compete, but will use the data provided (along with that from external sources) and results to analyse their performance.
Kaggle provides some data, which includes player, team, season, and tournament results from the past 18 NCAA seasons, but is not sufficient for generating good predictions.
Therefore, other data sources will be incorporated to hopefully create more powerfull models. The features used were obtained from Ken Pomeroy, NCAA basketball statstician.
The researchers will use Random Forests and Artificial Neural Networks, to predict winners and associated probability of occurence for each round of play, for the 64 teams.
<Previous seasons will assist in the process of training and testing, and results benchmarked against specific seasons and Kaggle competition results.>


\section{Data Abstraction, Features, and Structure}
The data used for creating predictions was gathered from multiple sources. Firstly, the Kaggle competition data served as a base of information, giving seasons, teams, game results, and tournament matchups.
Next, statistics were integrated into to this dataset, representing the features for each team. 
These were obtained from renowned NCAA statstician, Ken Pomeroy (also known as ``kenpom'').
KenPom has extensive data available on every NCAA Division I basketball team, which serves as the basis for learning in this project.
The researchers were only able to obtain team-wide data, but in the future could potentially utilize KenPom's player-specific data as well. 
A subset of the statistics used is seen below. 
A total of 30 statistics from KenPom are used to represent each team, whose meaning can be located within KenPom's blog \cite{KenPom}. 
In both approaches it is not necessary that the researchers know or understand what the values are, just that they accurately represent a team according to KenPom.

\begin{itemize}
  \item Tempo
  \item Adjusted Tempo
  \item Offensive Efficiency
  \item Defensive Efficiency
  \item Pythagorean Strength of Schedule
  \item Offensive Rebounding Percentage
  \item Free Throw Rate
  \item Effective Field Goal Percentage Against
\end{itemize}

%\begin{longtable}[H] {|p{1cm}|p{1cm}|p{1cm}|}
%\hline
%    Tempo                                   & Adjusted Tempo                  & Offensive Efficiency                    \\ \hline
%    Adjusted Offensive Efficiency           & Defensive Efficiency            & Adjusted Defensive Efficiency           \\ \hline
%    Pythagorean Strength of Schedule        & Effective Field Goal Percentage & Effective Field Goal Percentage         \\ \hline
%    Turnover Percentage                     & Offensive Rebounding Percentage & Free Throw Rate                         \\ \hline
%    Effective Field Goal Percentage Against & Turnover Percentage Against     & Offensive Rebounding Percentage Against \\ \hline
%    \label{tab:stats}
%	\caption{Integration tests for Relay Framework}
%\end{longtable}

The researchers discussed running a feature reduction process, such as Principle Component Analysis (PCA), but ultimately decided it is not currently needed. The systems, ANN and RF, ran well with the current set of features and dimensionality reduction was not needed. However, if more features are added, as proposed later in the report, it is advised that some feature analysis is performed. The researchers suggest using: PCA, step-wise regression, or python libraries, such as scikit learn, to select the features with highest predictive power.

\subsection{Prediction Format}
There are two formats for the predictions generated by the models: full tournament and bracket. Firstly, the full tournament predictions are for every potential matchup in that tournament. There are a potential 2278 combinations of teams in the March Madness tournament and generating a prediction for each is necessary for evaluating against the actual tournament results. This is also in accordance with the submission format required for the Kaggle competition. An example of this is seen in Table~\ref{tab:kaggle}. The structure of the game is: season, winning team id, losing team id. Secondly, the researchers will generate a standard bracket for the tournament. This will utilize the models to select a winner for each round of the tournament. 
Obviously, if the predictor makes an incorrect prediction early on in the tournament, then subsequent predictions will also be incorrect -- propogating through the tournament. <AND SOMETHING ELSE>.

\begin{table}[H]
\begin{centering}
    \begin{tabular}{|l|l|}
    \hline
    Game        & Prediction \\ \hline
    R\_507\_509 & 0.63       \\ 
    R\_507\_511 & 0.75       \\ 
    R\_507\_512 & 0.55       \\ 
    R\_507\_521 & 0.80       \\ 
    R\_507\_536 & 0.91       \\ \hline
    \end{tabular}
    \label{tab:kaggle}
    \caption{Snippet of Sample Submission for Kaggle}
    \end{centering}
\end{table}

\subsection{Testing Methodology}
Three different tests were conducted to evaluate the performance of the learning techniques. Firstly, <IDK>. Next, a traditional bracket is constructed in the same form as that in Appendix~\ref{ap:bracket}. The bracket's performance is then evaluated against the actual tournanment results and percent correct is calculated (i.e. how many games were correctly predicted out of all games played). Lastly, a full tournament is constructed, explained in Prediction Format above, and log loss  of predictions is calculated. Only games that actually occured are included in the calculation and this metric allows for comparison against the Kaggle results. Note that a direct comparison cannot be made as the data used only extends to the 2012-2013 season.

\section{Random Forest}
The first method used is Random Forests. Random Forests combine the benefits of using simple decision trees and the complexity of esemble learning methods which consistently outperforms single decision trees \cite{THOMASG.DIETTERICH:1999}. They are a collection of decision tress that contribute to an overall class vote. The single decision trees will in the ensemble process will not undergo any pruning as it could have negative consequences \cite{THOMASG.DIETTERICH:1999}. The class assignment is determined by:
\begin{equation}
class = argmax(p_1, p_2,...,p_i)
\end{equation}
These are otherwise known as probability estimation trees \cite{THOMASG.DIETTERICH:1999}. The ability to assign ranks to class assignments can be used to measure the accuracy of correct classification. This is beneficical because it allows the researchers to extract the probabilities of each class assignment. 

The Random Forest is constructed using python's Sklearn package. The classification will undergo the three testing methods as described previously. In the cross-validation testing the Random Forest parameters will be tweaked and used in all subsequent tests. Three parameters are chosen to alter the performance of the Random Forest:
\begin{itemize}
\item the number of trees in the forest ($n\_estimators$)
\item the number of jobs to run in parallel for both fit and predict ($n\_jobs$)
\item the maximum depth of the tree ($max\_depth$)
\end{itemize}

The cross-validation performance and the corresponding parameter values can be found in Table ~\ref{tab:RF_CV}.

\begin{table}[H]
	\centering
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Accuracy} & \textbf{n\_jobs} & \textbf{n\_estimators} & \textbf{max\_depth} \\ \hline
    56\%     & 1       & 10            & NONE       \\ \hline
    58\%     & 1       & 5             & 10         \\ \hline
    59\%     & 1       & 15            & 5          \\ \hline
    60\%     & 2       & 15            & 5          \\ \hline
    59\%     & 2       & 15            & 15         \\ \hline
    59\%     & 2       & 15            & NONE       \\ \hline
    \end{tabular}
    \caption {Random Forest Cross-Validation}
    \label{tab:RF_CV}
\end{table}

The cross-validation test was carried out by setting 30\% of the training data to act as the testing data. The percentage was then calculated based on the number of correctly predicted matchups (predicting correctly when team A or team B wins) divided by the total number of matchups (including false negatives and false positive errors).  Based on the cross-validation, the Random Forest performs with approximately 60\% accuracy with parameters $n\_jobs$ = 2, $n\_estimators$ = 15, and $max\_depth$ = 5. The Random Forest classifier uses these values in all subsequent simulations.

Increasing the number of jobs allows the computer to utilize both cores for both fitting and predicting, and so the class assignment can run in tandem with the construction of each decision tree. Increasing the number of trees in the forest assists in a more accurate prediction of class probabilities by decreasing the effect of a non-congruous prediction of a single tree.

The Log Loss value was tested using the entire tournament. A vector of probabilities was produced for each team summing to 1 and the value calculated. Due to the randomness assoicated with Random Forests multiple simulations were run as seen in Table~\ref{tab:ll_table}.

\begin{table}[H]
	\centering
    \begin{tabular}{|l|l|l|}
    \hline
    ~   \textbf{Simulation Number}  & \textbf{Log Loss} \\ \hline
    Smulation 1 & 0.611  \\ \hline
    Smulation 2 & 0.637\\ \hline
    Smulation 3 &  0.626\\ \hline
    \end{tabular}
    \caption {Tournament Prediction Simulations}
    \label{tab:ll_table}
\end{table}

Similarly, the overall tournament accuracy for the 2013 season was run 5 different times to account for the randomness in the model. The results can be found in Table~\ref{tab:thetable}.

The distribution of the simulations is approximately 54\%. The accuracy is lower than the training validation but not by a large amount. This results from the large number of upsets in the 2013 tournament compared to previous years. 
 
\begin{table}[H]
	\centering
    \begin{tabular}{|l|l|l|}
    \hline
    ~   \textbf{Simulation Number}  & \textbf{Accuracy} \\ \hline
    Smulation 1 & 54\%  \\ \hline
    Smulation 2 & 58\% \\ \hline
    Smulation 3 & 52\% \\ \hline
    Smulation 4 & 51\% \\ \hline
    Smulation 5 & 57\%\\ \hline
    \end{tabular}
    \caption {Tournament Prediction Simulations}
    \label{tab:thetable}
\end{table}

%\begin{figure}[htb]
%\centering
%\includegraphics[width=2in]{ANN.jpeg}
%\caption{This is a sample figure.}
%\label{fig_ANN}
%\end{figure}

%This is a sample equation:
%\begin{equation}
%f(x) = \frac{\sin(x^2)-1}{\sum\limits_{i=1}^{n}i^3-i}+\log_2 x
%\end{equation}


\section{Neural Network}
The second method uses a multi-layer, feedforward, backpropagation artificial neural network to simulate and predict the results of the NCAA basketball tournament.
In the sub-field of data classification, neural-network methods have been found to be useful alternatives to statistical techniques such as those which involve regression analysis or probability density estimation (e.g., Holmstrom et al., 1997).
Individual nodes in a neural network emulate biological neurons by taking input data and performing simple operations on the data, selectively passing the results on to other neurons in other (hidden) layers.
The neural network used is composed of 1) an input layer of nodes, 2) one or more intermediate (hidden) layers of nodes, and 3) an output layer of nodes (in this case one)
For clarity, it is often best to describe a particular network by its number of layers, and the number of nodes in each layer (e.g., a "4-3-2" network has an input layer with 4 nodes, a single hidden layer with 3 nodes, and an output layer with 2 nodes).
The following section will use this naming convention to describe the different networks used.
However primarily, the network will have 60 input nodes (as previously described by in Features), and a single output node -- following the form "60-H-1".
Where H is the number of neurons in a single hidden layer.

Choosing the proper type of neural network for a certain problem can be a critical issue.
Varying the number of neurons, and the number of hidden layers allows adjustment in the operation and performance of the neural network.
Additional definitions of a good model include: activation functions and the training function[B1].
The adjustment of these parameters varies with every application.
The approach follows a number of different combinations of adjusting these parameters to get the best result.
However, too many neurons with too many hidden layers may lead to over-fitting the data -- which will cause poor performance when testing.
The researchers explore the implementation of the described neural network using the $pybrain$ library for Python.
The $pybrain$ library offers a robust, and relatively straight-forward network, but is known for being comparatively slow.

%The backpropagation algorithm employs the Delta Rule, calculating error at output units.
%The effects of error in the output node(s) are propagated backward through the network after each training case.
%The essential idea of backpropagation is to combine a non-linear multi-layer perceptron-like system capable of making decisions with the objective error function of the Delta Rule (McClelland and ;Rumelhart, 1988).

<<<<<<< HEAD
%In the employment of the backpropagation algorithm, each iteration of training involves the following steps: 
%\begin{enumerate}
%\item A particular case of training data is fed through the network in a forward direction, producing results at the output layer
%\item Error is calculated at the output nodes based on known target information, and the necessary changes to the weights that lead into the output layer are determined based upon this error calculation
%\item  The changes to the weights that lead to the preceding network layers are determined as a function of the properties of the neurons to which they directly connect (weight changes are calculated, layer by layer, as a function of the errors determined for all subsequent layers, working backward toward the input layer) until all necessary weight changes are calculated for the entire network. 
%\end{enumerate}

%The calculated weight changes are then implemented throughout the network, the next iteration(epoch) begins, and the entire procedure is repeated using the next training pattern.
=======
In the employment of the backpropagation algorithm, each pass of training involves the following steps: 
\begin{enumerate}
\item A particular case of training data is fed through the network in a forward direction, producing results at the output layer
\item Error is calculated at the output nodes based on known target information, and the necessary changes to the weights that lead into the output layer are determined based upon this error calculation
\item  The changes to the weights that lead to the preceding network layers are determined as a function of the properties of the neurons to which they directly connect (weight changes are calculated, layer by layer, as a function of the errors determined for all subsequent layers, working backward toward the input layer) until all necessary weight changes are calculated for the entire network. 
\end{enumerate}

The calculated weight changes are then implemented throughout the network, the next pass begins, and the entire procedure is repeated using the next training pattern until all training patterns are fed through the network.
For each time all training data is fed through the network, this is one Epoch.
The $pybrain$ documentation suggests running a number of training iterations for a defined number of epochs to build a good model for the network.
>>>>>>> a1e18c26aa2ac675969173ef4b4989c0d58ad266

The final component of the neural network used is a bias for the system.
A bias is commonly visualized as simply values associated with each node in the hidden and output layers of a network, but in practice are treated in exactly the same manner as other weights, with all biases simply being weights associated with vectors that lead from a single node whose location is outside of the main network and whose activation is always 1.

Four network structure variables are adjusted for assessing the performance of the network:
\begin{itemize}
\item Number training iterations ($\eta$)
\item Number epochs ($\epsilon$)
\item Number hidden layers ($\mu$)
\item Number neurons in each hidden layer($H$)
\end{itemize}

After each adjustment to the four variables, statistics are collected on training and testing error for the entire dataset.
The network is then cross-validated and Mean-Squared Error (MSE) is returned as an initial performance metric following the initial testing and validation protocol.
The purpose of training is to reduce MSE to a reasonably low value in as few epochs as possible.
When training is sufficiently long, the plot of MSE will asymptotically decrease to a horizontal straight line at $MSE = 0$.
Secondary, the network is given all possible match-ups in a tournament to run test type 1 and return the log-loss of the prediction.
Finally, a bracket is predicted and the prediction accuracy is returned as shown in (INSERT EQ REF TO numCorrect/numGames).

Table \ref{tab:nn-sims} provides a small subset of the architecture types attempted, but are chosen to represent the variation in model results shown in Table \ref{tab:nn-error} and \ref{tab:nn-pred}.
\begin{table}[H]
	\centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    ~    \textbf{Simulation Number} & \textbf{$\mu$} & \textbf{$H$} & \textbf{$\eta$} & \textbf{$\epsilon$}\\ \hline
    Simulation 1 & 2 & 30 & 10 & 5\\ \hline
    \end{tabular}
    \caption {Neural Network Architecture Simulations}
    \label{tab:nn-sims}
\end{table}

The following table summarizes the results for initial testing from a subset of the attempts on network structure (using the "60-H-1" structure, where there can be $\mu$ number of hidden layers with $H$ neurons in each).

\begin{table}[H]
	\centering
    \begin{tabular}{|l|l|l|l|}
    \hline
    ~   \textbf{Simulation} & \textbf{Training Error} & \textbf{Testing Error} & \textbf{MSE}\\ \hline
    Simulation 1 & 34\% & 30\% & 20\% \\ \hline
    \end{tabular}
    \caption {Neural Network Architecture Error \& Cross-Validation (MSE)}
    \label{tab:nn-error}
\end{table}


The following table summarizes the results from the same subset of attempts on network structure towards log-loss, and tournament bracket prediction tests.
\begin{table}[H]
	\centering
    \begin{tabular}{|l|l|l|}
    \hline
    ~   \textbf{Simulation} & \textbf{Log-Loss} & \textbf{Accuracy}\\ \hline
    Simulation 1 & .6 & 30\%\\ \hline
    \end{tabular}
    \caption {Neural Network: Tournament Prediction Results}
    \label{tab:nn-pred}
\end{table}

The most simple way to explore the differences in methods between random forests and neural networks is comparing bracket tournament prediction by game, accuracy, and log-loss values.
The following section provides a thorough summary. 
\section{Results}

\section{Future Work and Conclusions}
%incorportating player statistics
%using some type of weighting function or window to calculate the aggregate metrics of each team
%incorporate ``meta-strats'' (i.e. manually changing things within the bracket). Didn't do this because it is not MI, but if we had a specific strategy in mind we could alter the results of the MI process (e.g. make all florida state games 1.0 for tehm if we think they are going to win because we get the most benefit from this). This leads to more gambling
%

The researchers believe that some improvements could be made to the model to better capture a teams current performance, and hopefully generate more accurate predictions. However, NCAA basketball is known for its high uncertainty in outcomes (i.e. large number of ``upsets''), so creating a more comprehensive model may not lead to better predictions. Therefore, further training and testing would need to be performed to validate these hypotheses.

Currently the features used are team-wide and do not include any player specific information. This could be important in prediction as ...

Additionally, features are calculated over the entire season and therefore are not necessarily indicative of a team during the season. The reseachers firsty propose the use of a weighting function. This would gradually decrease the strength of statistics from the beginning of the season (and prior season for early predictions) as it progresses, attempting to better capture the current performance of each team. Secondly, a ``window'' could be used to tabulate the aggregate metrics, meaning that the last $n$ games would be valid for the current prediction. This again attempts to more accurately capture current team performance. The season-wide statistics mask performance nuances and using either of these methods could better represent the growth of a team over the season and how it contributes to their results over time.

Next, while this does not necesarilly pertain to machine learning, <``meta-strategies''> may boost log-loss performance. An example of such a strategy is, if a weaker-seeded team is considered in later rounds, this means they beat stronger-seeds and therefore are a higher performing team than initially determined. This is useful information to for making predictions and could be incorporated as a feature in the model, measuring ``perceived strength'', ``updated seed'', or some other power rating. This also does not solely apply to weak-seeds. While this power rating would have a larger impact on weaker teams, it would affect any team making it further in the tournament, rewarding them for their performance. <maybe another sentence>

Lastly, the two prediction methods above (full tournament and bracket) could be used in tandem to generate a ``custom'' full tournament prediction. The bracket outlines the researches selected predictions, <BASED ON>, and the full tournament can be modified to reflect this. Each win in the bracket essentially represents a 1.0 likelihood prediction for the winning team in that game. To acheive maximum benefit for this potential win, the full tournament should reflect this. While this approach can lead to greater losses, <it reflects the best estimates of the model, something about our best guesses based on our models>.

% use section* for acknowledgement
\section*{Acknowledgment}
The authors would like to thank


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\bibliography{references}
%\begin{thebibliography}{1}

%\end{thebibliography}

\end{document}


