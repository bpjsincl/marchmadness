%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}

% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


\usepackage[cmex10]{amsmath}

% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/




%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/

% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.


%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.


\usepackage{float}


% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{longtable}
\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Predicting NCAA March Madness (Brackets, Game Outcomes) (with MI Techniques)}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{
\IEEEauthorblockN{Brian Sinclair}
\IEEEauthorblockA{Systems Desing Engineering\\
University of Waterloo\\
Waterloo, Canada\\
Email: bpjsincl@uwaterloo.ca}
\and
\IEEEauthorblockN{Matthew Chong}
\IEEEauthorblockA{Systems Desing Engineering\\
University of Waterloo\\
Waterloo, Canada\\
Email: mt2chong@uwaterloo.ca}
\and
\IEEEauthorblockN{D. Scott Neil}
\IEEEauthorblockA{Systems Desing Engineering\\
University of Waterloo\\
Waterloo, Canada\\
Email: dsneil@uwaterloo.ca}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% make the title area
\maketitle


\begin{abstract}
%Bracket and Game prediction is continually growing in popularity due to revenue production over various gambling mediums.
March Madness is a sporting event held every year, with competing Division I NCAA college basketball teams, and attracts significant gambling and competition to predict winners.
Outside of the typical gambling realm, Kaggle held a competition this season, offering \$15,000 to the winner.
The researchers did not participate, but used this competition, its data and results, as a foundation for the analysis.
Many tactics are currently being reasearched and employed, such as: ANNs, varying regression techniques, probabalistic optimization, SVM's, and genetic algorithms.
However, due to high uncertainty in outcomes, none have been extremely accurate.
The researchers applied Artificial Neural Networks and Random Forests, with data gathered from Kaggle and Ken Pomeroy.
Using these techniques, the researchers attempted to predict the winners of this tournament, constructing a traditional bracket and outcome of each potential matchup with a percentage confidence.
The previous seasons assisted in the process of training and testing, and models were benchmarked against specific seasons.
Additionally, to determine a techniques success, Log Loss of predictions was calculated and compared against Kaggle results.
Through the analysis the researchers found that the Random Forest is the superior method, with the best bracket prediction accuracy of 58% and log loss of 0.611.
\end{abstract}


\section{Introduction}
March Madness is comprised of the top 64 teams in the United States.
It is structured in a classical tournament bracket format, beginning with 32 games (64 teams) and ending with one final Championship game with all single-round knockouts.
The losing team from each game is eliminated from the tournament at each round, which cuts the number of games at each successive round in half.
This provides a tournament layout consisting of 6 rounds.
Betting on this tournament is extremely popular and a total of \$12 billion was spent on gambling related bets in 2012 \cite{BradTuttle-2012}. 
The most popular bet made comes in the form of the March Madness bracket. So popular that more Americans filled out a bracket in 2012 than voted in the Presidential Election \cite{DavidHolmes-2013}. 
Predicting this bracket includes choosing the winner of each game in the tournament for each of the six rounds. 
The difficulty of predicting a perfect March Madness bracket is widely known and accepted and Warren Buffet has even offered a \$1 billion cash prize to anyone who is successful \cite{wbuff_bill}.

Attempts to perfectly predict March Madness results using Machine Learning techniques has seen a recent surge in popularity. 
Hand calculated predictions used to be the norm, but more advanced MI techniques are starting to be used with the wealth of data now available online. 
However, there is no greed upon or industry standardized method for predicting a bracket, or even the outcome of a basketball game. 
Many tactics are used, such as: neural nets \cite{KarlLeswing}, varying regression and probabalistic optimization techniques \cite{adams-dahl-murray-2010a, DannyTarlow, BradyWest, BradleyCarlin, ProbModelsNCAA-1991, MoreProbModelsNCAA-1996}, support vector machines \cite{KennethDeakins}, and genetic algorithms \cite{ScottTurner}, but due to high uncertainty in outcomes, none have been extremely accurate, and continuous improvements are being made.

Kaggle, a machine-learning competition repository, held a competition this year for predicting the outcome of each possible match-up in the March Madness tournament (\url{https://www.kaggle.com/c/march-machine-learning-mania}).
The researchers did not compete, but will use the data provided (along with that from external sources) and results to analyse their performance.
Kaggle provides some data, which includes player, team, season, and tournament results from the past 18 NCAA seasons, but is not sufficient for generating good predictions.
Therefore, other data sources will be incorporated to hopefully create more powerful models. The features used were obtained from Ken Pomeroy {\url{http://kenpom.com/}), NCAA basketball statistician.
The researchers will use Random Forests and Artificial Neural Networks, to predict winners and associated probability of occurrence for each round of play, for the 64 teams.
Previous seasons will assist in the process of training and testing, and results benchmarked against specific tournament results, and Kaggle competition results.

\section{Data Abstraction, Features, and Structure}
The data used for creating predictions was gathered from multiple sources. Firstly, the Kaggle competition data served as a base of information, giving seasons, teams, game results, and tournament matchups.
Next, statistics were integrated into to this dataset, representing the features for each team. 
These were obtained from renowned NCAA statstician, Ken Pomeroy (also known as ``kenpom'').
KenPom has extensive data available on every NCAA Division I basketball team, which serves as the basis for learning in this project.
The researchers were only able to obtain team-wide data, but in the future could potentially utilize KenPom's player-specific data as well. 
A subset of the statistics used is seen below. 
A total of 30 statistics from KenPom are used to represent each team, whose meaning can be located within KenPom's blog \cite{KenPom}. 
In both approaches it is not necessary that the researchers know or understand what the values are, just that they accurately represent a team according to KenPom.

\begin{itemize}
  \item Tempo
  \item Adjusted Tempo
  \item Offensive Efficiency
  \item Defensive Efficiency
  \item Pythagorean Strength of Schedule
  \item Offensive Rebounding Percentage
  \item Free Throw Rate
  \item Effective Field Goal Percentage Against
\end{itemize}

The researchers discussed running a feature reduction process, such as Principle Component Analysis (PCA), but ultimately decided it is not currently needed. 
The systems, ANN and RF, ran well with the current set of features and dimensionality reduction was not needed, as there are only a total of 60 features.
However, feature reduction would have provided a slight increase in speed, but the research done on the tools used for learning in $sciikit-learn$ and $pybrain$ suggest that it would not be noticeable.
Additionally, if more features are added, as proposed later in the report, it is advised that some feature analysis is performed.
The researchers suggest using: PCA, step-wise regression, in python libraries, such as $sciikit-learn$, to select the features with highest predictive power.

\subsection{Prediction Format}
There are two formats for the predictions generated by the models: full tournament and bracket. Firstly, the full tournament predictions are for every potential match-up in that tournament. 
There are a potential 2278 combinations of teams in the March Madness tournament and generating a prediction for each is necessary for evaluating against the actual tournament results. This is also in accordance with the submission format required for the Kaggle competition.
An example of this is seen in Table~\ref{tab:kaggle}. The structure of the game is: season, winning team id, losing team id. Secondly, the researchers will generate a standard bracket for the tournament. This will utilize the models to select a winner for each round of the tournament. 
Obviously, if the predictor makes an incorrect prediction early on in the tournament, then subsequent predictions will also be incorrect -- propagating through the tournament. <AND SOMETHING ELSE>.  %scott I'll leave this for you. not sure what you want here

\begin{table}[H]
\begin{centering}
    \begin{tabular}{|l|l|}
    \hline
    Game        & Prediction \\ \hline
    R\_507\_509 & 0.63       \\ 
    R\_507\_511 & 0.75       \\ 
    R\_507\_512 & 0.55       \\ 
    R\_507\_521 & 0.80       \\ 
    R\_507\_536 & 0.91       \\ \hline
    \end{tabular}
    \label{tab:kaggle}
    \caption{Snippet of Sample Submission for Kaggle}
    \end{centering}
\end{table}

\subsection{Testing Methodology}
Three different tests were conducted to evaluate the performance of the learning techniques.
Initially, it is first important to evaluate the performance of the classifier based off of training error and testing error.
Above, the absolute error that is returned, the researchers will cross-validate using Mean-Squared Error (MSE).
This is used to check for the presence of over-fitting (or over-training) data in the solution.
Next, a traditional bracket is constructed in the same form as that in Appendix~\ref{ap:bracket}. The bracket's performance is then evaluated against the actual tournament results and percent correct is calculated (i.e. how many games were correctly predicted out of all games played).
\begin{equation}
accuracy = numCorrect/numGames
\label{pred-accuracy}
\end{equation}
Lastly, a full tournament is constructed, explained in Prediction Format above, and log loss of predictions is calculated using all possible match-ups. 
Only games that actually occurred are included in the calculation and this metric allows for comparison against the Kaggle results. 
Note that a direct comparison cannot be made as the data used only extends to the 2012-2013 season.

\section{Random Forest}
The first method used is Random Forests. 
Random Forests combine the benefits of using simple decision trees and the complexity of ensemble learning methods which consistently outperforms single decision trees \cite{THOMASG.DIETTERICH:1999}.
They are a collection of decision tress that contribute to an overall class vote.
The single decision trees will in the ensemble process will not undergo any pruning as it could have negative consequences \cite{THOMASG.DIETTERICH:1999}. 
The class assignment is determined by:
\begin{equation}
class = argmax(p_1, p_2,...,p_i)
\end{equation}
These are otherwise known as probability estimation trees \cite{THOMASG.DIETTERICH:1999}.
The ability to assign ranks to class assignments can be used to measure the accuracy of correct classification.
This is beneficial because it allows the researchers to extract the probabilities of each class assignment. 

The Random Forest is constructed using python's $sciikit-learn$ package. 
The classification will undergo the three testing methods as described previously. 
In the cross-validation testing the Random Forest parameters will be tweaked and used in all subsequent tests. 
Three parameters are chosen to alter the performance of the Random Forest:
\begin{itemize}
\item the number of trees in the forest ($n\_estimators$)
\item the number of jobs to run in parallel for both fit and predict ($n\_jobs$)
\item the maximum depth of the tree ($max\_depth$)
\end{itemize}

The cross-validation performance and the corresponding parameter values can be found in Table ~\ref{tab:RF_CV}.

\begin{table}[H]
	\centering
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Accuracy} & \textbf{n\_jobs} & \textbf{n\_estimators} & \textbf{max\_depth} \\ \hline
    56\%     & 1       & 10            & NONE       \\ \hline
    58\%     & 1       & 5             & 10         \\ \hline
    59\%     & 1       & 15            & 5          \\ \hline
    60\%     & 2       & 15            & 5          \\ \hline
    59\%     & 2       & 15            & 15         \\ \hline
    59\%     & 2       & 15            & NONE       \\ \hline
    \end{tabular}
    \caption {Random Forest Cross-Validation}
    \label{tab:RF_CV}
\end{table}

The cross-validation test was carried out by setting 30\% of the training data to act as the testing data.
The percentage was then calculated based on the number of correctly predicted match-ups (predicting correctly when team A or team B wins) divided by the total number of match-ups (including false negatives and false positive errors). 
Based on the cross-validation, the Random Forest performs with approximately 60\% accuracy with parameters $n\_jobs$ = 2, $n\_estimators$ = 15, and $max\_depth$ = 5.
The Random Forest classifier uses these values in all subsequent simulations.

Increasing the number of jobs allows the computer to utilize both cores for both fitting and predicting, and so the class assignment can run in tandem with the construction of each decision tree. 
Increasing the number of trees in the forest assists in a more accurate prediction of class probabilities by decreasing the effect of a non-congruous prediction of a single tree.

The Log-Loss value was tested using the entire tournament.
A vector of probabilities was produced for each team summing to 1 and the value calculated.
Due to the randomness associated with Random Forests multiple simulations were run as seen in Table~\ref{tab:ll_table}.

\begin{table}[H]
	\centering
    \begin{tabular}{|l|l|l|}
    \hline
    ~   \textbf{Simulation Number}  & \textbf{Log Loss} \\ \hline
    Simulation 1 & 0.611  \\ \hline
    Simulation 2 & 0.637\\ \hline
    Simulation 3 &  0.626\\ \hline
    \end{tabular}
    \caption {Tournament Prediction Simulations}
    \label{tab:ll_table}
\end{table}

Similarly, the overall tournament accuracy for the 2013 season was run 5 different times to account for the randomness in the model. The results can be found in Table~\ref{tab:thetable}. 
The accuracy is computed using \eqref{pred-accuracy}.

The distribution of the simulations is approximately 54\%.
The accuracy is lower than the training validation but not by a large amount. 
This results from the fact that the 2013 tournament had the largest number of upsets in NCAA history next to the 2014 season \cite{BenZauzmer:2014}.
 
\begin{table}[H]
	\centering
    \begin{tabular}{|l|l|l|}
    \hline
    ~   \textbf{Simulation Number}  & \textbf{Accuracy} \\ \hline
    Simulation 1 & 54\%  \\ \hline
    Simulation 2 & 58\% \\ \hline
    Simulation 3 & 52\% \\ \hline
    Simulation 4 & 51\% \\ \hline
    Simulation 5 & 57\%\\ \hline
    \end{tabular}
    \caption {Tournament Prediction Simulations}
    \label{tab:thetable}
\end{table}

Going forward, different parameters could have been incorporated into the training of the model. Controlling the number of times a leaf has been split, or the number of samples that is required to split a leaf could have direct impacts on the results of the decision tree. Implementing these features would yield better results if the training size incorporated a greater number of years, which is the reason why these features remained untouched. Increasing the data set would thereby increase the number of parameters that could be optimized. 

%\begin{figure}[htb]
%\centering
%\includegraphics[width=2in]{ANN.jpeg}
%\caption{This is a sample figure.}
%\label{fig_ANN}
%\end{figure}

%This is a sample equation:
%\begin{equation}
%f(x) = \frac{\sin(x^2)-1}{\sum\limits_{i=1}^{n}i^3-i}+\log_2 x
%\end{equation}

%\begin{figure}[htb]
%\centering
%\includegraphics[width=2in]{ANN.jpeg}
%\caption{This is a sample figure.}
%\label{fig_ANN}
%\end{figure}

%This is a sample equation:
%\begin{equation}
%f(x) = \frac{\sin(x^2)-1}{\sum\limits_{i=1}^{n}i^3-i}+\log_2 x
%\end{equation}


\section{Neural Network}
The second method uses a multi-layer, feedforward, backpropagation artificial neural network to simulate and predict the results of the NCAA basketball tournament.
In the sub-field of data classification, neural-network methods have been found to be useful alternatives to statistical techniques such as those which involve regression analysis or probability density estimation (e.g., \cite{Holmstrom-1192}).
Individual nodes in a neural network emulate biological neurons by taking input data and performing simple operations on the data, selectively passing the results on to other neurons in other (hidden) layers.
The neural network used is composed of 1) an input layer of nodes, 2) one or more intermediate (hidden) layers of nodes, and 3) an output layer of nodes (in this case, one).
For clarity, it is often best to describe a particular network by its number of layers, and the number of nodes in each layer (e.g., a "4-3-2" network has an input layer with 4 nodes, a single hidden layer with 3 nodes, and an output layer with 2 nodes).
The researchers will use this naming convention to describe the different networks used.
However primarily, the network will have 60 input nodes (as previously described by in Features), and a single output node -- following the form "60-H-1".
Where $H$ is the number of neurons in a single hidden layer.

Choosing the proper type of neural network for a certain problem can be a critical issue.
Varying the number of neurons, and the number of hidden layers allows adjustment in the operation and performance.
Additional adjustment to create a good model include: activation functions and the output function\cite{moustafa}. 
The adjustment of these parameters varies with every application of a neural network -- defined below.
The approach follows a number of different combinations of adjusting these parameters to get the best result.
Caution must be used in manipulating characteristics as too many neurons, with too many hidden layers may lead to over-fitting the data -- which will cause poor performance when testing.

The researchers explore the implementation of the described neural network using the $PyBrain$ library for Python \cite{PyBrain}.
The $PyBrain$ library offers a robust, and relatively straight-forward network construction, but is known for being comparatively slow.
The network is built using $buildNewtork$, setting the hidden classes as $TahnLayers$ and output function class as a $SoftMaxLayer$.

The final component of the neural network used is a bias for the system.
A bias is commonly visualized as simply values associated with each node in the hidden and output layers of a network, but in practice are treated in exactly the same manner as other weights, with all biases simply being weights associated with vectors that lead from a single node whose location is outside of the main network and whose activation is always 1.

Four network structure variables are adjusted for assessing the performance of the network:
\begin{itemize}
\item Number training iterations ($\eta$)
\item Number epochs ($\epsilon$)
\item Number hidden layers ($\mu$)
\item Number neurons in each hidden layer($H$)
\end{itemize}

After the network is constructed with the desired characteristics, the data is split for later cross-validation, and trainer set up.
Same as with the Random Forest technique, 70\% of data is used for training, and 30\% for testing. 
That same 30\% is used for cross-validation.
$BackPropTrainer$ is used with its generic construction except for setting the $weightdecay$ parameter to 0.01.
As mentioned, the training data contains all features from season 2010 to 2013.
In introducing a $weightdecay$ of non-zero for features from earlier seasons, the network reduces their impact.
The essential idea of backpropagation is to combine a non-linear multi-layer perceptron-like system capable of making decisions with the objective error function of the Delta Rule \cite{McClelland-1988}.
The network is trained with the desired number of epochs and iterations, and performance statistics collected.
For each time all training data is fed through the network, this is one Epoch.
The $pybrain$ documentation suggests running a number of training iterations for a defined number of epochs to build a good model for the network.

The network is then cross-validated and Mean-Squared Error (MSE) is used as an initial performance metric following the initial testing and validation protocol.
The purpose of training is to reduce MSE to a reasonably low value in as few epochs as possible.
When training is sufficiently long, the plot of MSE will asymptotically decrease to a horizontal straight line at $MSE = 0$.
Secondary, the network is given all possible match-ups in a tournament to run test type 1 and return the log-loss of the prediction.
Finally, a bracket is predicted and the prediction accuracy is returned as shown in \eqref{pred-accuracy}.

%The backpropagation algorithm employs the Delta Rule, calculating error at output units.
%The effects of error in the output node(s) are propagated backward through the network after each training case.
%The essential idea of backpropagation is to combine a non-linear multi-layer perceptron-like system capable of making decisions with the objective error function of the Delta Rule (McClelland and ;Rumelhart, 1988).

%In the employment of the backpropagation algorithm, each pass of training involves the following steps: 
%\begin{enumerate}
%\item A particular case of training data is fed through the network in a forward direction, producing results at the output layer
%\item Error is calculated at the output nodes based on known target information, and the necessary changes to the weights that lead into the output layer are determined based upon this error calculation
%\item  The changes to the weights that lead to the preceding network layers are determined as a function of the properties of the neurons to which they directly connect (weight changes are calculated, layer by layer, as a function of the errors determined for all subsequent layers, working backward toward the input layer) until all necessary weight changes are calculated for the entire network. 
%\end{enumerate}

%The calculated weight changes are then implemented throughout the network, the next pass begins, and the entire procedure is repeated using the next training pattern until all training patterns are fed through the network.
%For each time all training data is fed through the network, this is one Epoch.
%The $pybrain$ documentation suggests running a number of training iterations for a defined number of epochs to build a good model for the network.

%After each adjustment to the four variables, statistics are collected on training and testing error for the entire dataset.

Table \ref{tab:nn-sims} provides a small subset of the architecture types attempted, but are chosen to represent the variation in model results shown in Table \ref{tab:nn-error} and \ref{tab:nn-pred}.
\begin{table}[H]
	\centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    ~    \textbf{Simulation Number} & \textbf{$\mu$} & \textbf{$H$} & \textbf{$\eta$} & \textbf{$\epsilon$}\\ \hline
    Simulation 1 & 2 & 30 & 10 & 5\\ \hline
    Simulation X & 3 & 25 & 5 & 5 \\ \hline
    Simulation Y & 2 & 30 & 30 & 1 \\ \hline
    Simulation Z & 2 & 30 & 50 & 1 \\ \hline
    \end{tabular}
    \caption {sNeural Network Architecture Simulations}
    \label{tab:nn-sims}
\end{table}

The following table summarizes the results for initial testing from a subset of the attempts on network structure (using the "60-H-1" structure, where there can be $\mu$ number of hidden layers with $H$ neurons in each).

\begin{table}[H]
	\centering
    \begin{tabular}{|l|l|l|l|}
    \hline
    ~   \textbf{Simulation} & \textbf{Training Error} & \textbf{Testing Error} & \textbf{MSE}\\ \hline
    Simulation 1 & 34\% & 30\% & 20\% \\ \hline
    Simulation X & ~ & ~ & ~ \\ \hline % all
    Simulation Y & 27\% & 27\% & 17\% \\ \hline % 2013
    Simulation Z & 26\% & 27\% & 18\% \\ \hline % 2013
    \end{tabular}
    \caption {Neural Network Architecture Error \& Cross-Validation (MSE)}
    \label{tab:nn-error}
\end{table}

%idk about: Mention some correlation to MSE>
Multiple simulations were run with significantly higher epochs and iterations (on the order of hundreds and thousands). 
In these tests, very unfavourable results were found. 
The log loss, testing error, and bracket incorrectness were generally much higher.
Analysing the results further lead to the discovering of these ``highly trained'' networks favouring extreme predictions (i.e. predictions greater than 99\%).
Incorrect extreme predictions are penalized heavily in the log loss calculation, which explained why this value was found to be much larger.
Another observation made was that predictions generally favoured the first team entered in the feature vector, which explained the lower bracket correctness.
The reaserchers conclude that high epochs and iterations lead to overtrainging of the networks and did not use any of the results generated from these tests.
Additionally, increasing the number of hidden layers beyond two was reasearched, but not tested.
Having many hidden layers presents a challenge for training and can easily lead to overfitting \cite{bengio2007scaling}.

The following table summarizes the results from the same subset of attempts on network structure towards log-loss, and tournament bracket prediction tests.

\begin{table}[H]
	\centering
    \begin{tabular}{|l|l|l|}
    \hline
    ~   \textbf{Simulation} & \textbf{Log-Loss} & \textbf{Accuracy}\\ \hline
    Simulation 1 & .6 & 30\%\\ \hline
    Simulation X & 0.65 & 49\% \\ \hline
    Simulation Y & 0.73 & 45\% \\ \hline
    Simulation Z & 0.71 & 45\% \\ \hline
    \end{tabular}
    \caption {Neural Network: Tournament Prediction Results}
    \label{tab:nn-pred}
\end{table}
< briefly mention best result and why it might be the best result>

The most simple way to explore the differences in methods between random forests and neural networks is comparing bracket tournament prediction by game, accuracy, and log-loss values.
The following section provides a thorough summary. 

\section{Results}
Table \ref{tab:comparison} shows the match-ups of the 2013 tournament. 
Each game has two teams and the actual winner, and then each method provides a prediction. 
The letter represents the division and the number represents the corresponding seed.
For a visual representation, Figure \ref{bracket} is provided, but for simplicity of comparison, the table is used.

From the table there are a few results that can be observed. 
One is that the Random Forest predicts 4 upsets in the first round. 
The seeding is the result of the team's statistics from the regular season play. 
The Random Forest uses these statistics and due to the large difference in seeding biases the higher ranked seed in the majority of cases. 
Due to the large number of upsets in the first round of the 2013 tournament, the Random Forest does not perform well. 
These errors then propagate throughout the rest of the tournment. 

Another observation that can be drawn from the predictions is that in the last three rounds of play there were only 5 total teams in the possible 15 total matchups that made it through in the predictions. 
In actual bracket predictions, it is more beneficial to predict teams in later rounds than in earlier rounds. 
With such a few number of predicted teams making it to later rounds, it would result in poor bracket performance amongst fantasy leagues.

\begin{figure}[H]
\centering
	\includegraphics[scale=.28]{bracket_blank.pdf}%
	\caption{Sample NCAA Tournament Bracket \cite{Andy-Soltis}}
	\label{bracket}
\end{figure}

\begin{table}[H]
	\centering
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    ~   \textbf{Slot}  & \textbf{team A} & \textbf{team B} & \textbf{Actual} & \textbf{RF Winner} & \textbf{NN Winner} \\ \hline
W16 & W16a & W16b & W16b & W16a &  NN  \\ \hline
Y11 & Y11a & Y11b & Y11b & Y11a &  NN  \\ \hline
Y16 & Y16a & Y16b & Y16b & Y16b &  NN  \\ \hline
Z13 & Z13a & Z13b & Z13b & Z13a &  NN  \\ \hline
R1W1 & W01 & W16 & W01 & W01 &  NN  \\ \hline
R1W2 & W02 & W15 & W02 & W02 &  NN  \\ \hline
R1W3 & W03 & W14 & W03 & W03 &  NN  \\ \hline
R1W4 & W04 & W13 & W04 & W04 &  NN  \\ \hline
R1W5 & W05 & W12 & W12 & W05 &  NN  \\ \hline
R1W6 & W06 & W11 & W06 & W11 &  NN  \\ \hline
R1W7 & W07 & W10 & W07 & W07 &  NN  \\ \hline
R1W8 & W08 & W09 & W09 & W08 &  NN  \\ \hline
R1X1 & X01 & X16 & X01 & X01 &  NN  \\ \hline
R1X2 & X02 & X15 & X15 & X02 &  NN  \\ \hline
R1X3 & X03 & X14 & X03 & X03 &  NN  \\ \hline
R1X4 & X04 & X13 & X04 & X04 &  NN  \\ \hline
R1X5 & X05 & X12 & X05 & X05 &  NN  \\ \hline
R1X6 & X06 & X11 & X11 & X11 &  NN  \\ \hline
R1X7 & X07 & X10 & X07 & X07 &  NN  \\ \hline
R1X8 & X08 & X09 & X08 & X09 &  NN  \\ \hline
R1Y1 & Y01 & Y16 & Y01 & Y01 &  NN  \\ \hline
R1Y2 & Y02 & Y15 & Y02 & Y02 &  NN  \\ \hline
R1Y3 & Y03 & Y14 & Y03 & Y03 &  NN  \\ \hline
R1Y4 & Y04 & Y13 & Y04 & Y04 &  NN  \\ \hline
R1Y5 & Y05 & Y12 & Y12 & Y05 &  NN  \\ \hline
R1Y6 & Y06 & Y11 & Y06 & Y06 &  NN  \\ \hline
R1Y7 & Y07 & Y10 & Y07 & Y07 &  NN  \\ \hline
R1Y8 & Y08 & Y09 & Y08 & Y09 &  NN  \\ \hline
R1Z1 & Z01 & Z16 & Z01 & Z01 &  NN  \\ \hline
R1Z2 & Z02 & Z15 & Z02 & Z02 &  NN  \\ \hline
R1Z3 & Z03 & Z14 & Z14 & Z03 &  NN  \\ \hline
R1Z4 & Z04 & Z13 & Z13 & Z04 &  NN  \\ \hline
R1Z5 & Z05 & Z12 & Z12 & Z05 &  NN  \\ \hline
R1Z6 & Z06 & Z11 & Z06 & Z06 &  NN  \\ \hline
R1Z7 & Z07 & Z10 & Z10 & Z07 &  NN  \\ \hline
R1Z8 & Z08 & Z09 & Z09 & Z08 &  NN  \\ \hline
R2W1 & W08 & W01 & W01 & W01 &  NN  \\ \hline
R2W2 & W07 & W02 & W02 & W02 &  NN  \\ \hline
R2W3 & W11 & W03 & W03 & W03 &  NN  \\ \hline
R2W4 & W05 & W04 & W04 & W04 &  NN  \\ \hline
R2X1 & X09 & X01 & X01 & X01 &  NN  \\ \hline
R2X2 & X07 & X02 & X15 & X02 &  NN  \\ \hline
R2X3 & X11 & X03 & X03 & X03 &  NN  \\ \hline
R2X4 & X05 & X04 & X04 & X04 &  NN  \\ \hline
R2Y1 & Y09 & Y01 & Y01 & Y01 &  NN  \\ \hline
R2Y2 & Y07 & Y02 & Y02 & Y07 &  NN  \\ \hline
R2Y3 & Y06 & Y03 & Y03 & Y06 &  NN  \\ \hline
R2Y4 & Y05 & Y04 & Y12 & Y04 &  NN  \\ \hline
R2Z1 & Z08 & Z01 & Z09 & Z08 &  NN  \\ \hline
R2Z2 & Z07 & Z02 & Z02 & Z07 &  NN  \\ \hline
R2Z3 & Z06 & Z03 & Z06 & Z03 &  NN  \\ \hline
R2Z4 & Z05 & Z04 & Z13 & Z05 &  NN  \\ \hline
R3W1 & W04 & W01 & W04 & W01 &  NN  \\ \hline
R3W2 & W03 & W02 & W03 & W03 &  NN  \\ \hline
R3X1 & X04 & X01 & X04 & X04 &  NN  \\ \hline
R3X2 & X03 & X02 & X15 & X02 &  NN  \\ \hline
R3Y1 & Y04 & Y01 & Y01 & Y04 &  NN  \\ \hline
R3Y2 & Y06 & Y07 & Y02 & Y07 &  NN  \\ \hline
R3Z1 & Z05 & Z08 & Z09 & Z08 &  NN  \\ \hline
R3Z2 & Z03 & Z07 & Z02 & Z03 &  NN  \\ \hline
R4W1 & W03 & W01 & W04 & W03 &  NN  \\ \hline
R4X1 & X02 & X04 & X04 & X02 &  NN  \\ \hline
R4Y1 & Y07 & Y04 & Y01 & Y07 &  NN  \\ \hline
R4Z1 & Z03 & Z08 & Z09 & Z08 &  NN  \\ \hline
R5WX & X02 & W03 & X04 & X02 &  NN  \\ \hline
R5YZ & Z08 & Y07 & Y01 & Y07 &  NN  \\ \hline
R6CH & Y07 & X02 & Y01 & Y07 &  NN  \\ \hline
    \end{tabular}
    \caption {Comparison of Predictions Between Random Forest and Neural Network}
    \label{tab:comparison}
\end{table}

\section{Future Work and Conclusions}
The researchers believe that some improvements could be made to the model to better capture a teams current performance, and hopefully generate more accurate predictions. 
However, NCAA basketball is known for its high uncertainty in outcomes (i.e. large number of ``upsets''), so creating a more detailed model of a team may not lead to better predictions.
Therefore, further training and testing would need to be performed to validate these hypotheses.

Currently the features used are team-wide and do not include any player specific information. 
This could be important in the prediction as often in a single game within the tournament a single player can change the outcome of the match-up.
For example, if a player has been playing extraordinarily well in the 10 games leading up to the tournament, this might be an important feature to include.
Conversely, if a key player becomes injured for the tournament then the team may not do as well as the statistics suggest.
Therefore, select player statistics might be an important addition to the feature range used for outcome prediction.

Additionally, features are calculated over the entire season and therefore are not necessarily indicative of a team during the season. 
The researchers firstly propose the use of a weighting function. 
This would gradually decrease the strength of statistics from the beginning of the season (and prior season for early predictions) as it progresses, attempting to better capture the current performance of each team. 
Secondly, a ``window'' could be used to tabulate the aggregate metrics, meaning that the last $n$ games would be valid for the current prediction. 
This again attempts to more accurately capture current team performance. 
The season-wide statistics mask performance nuances and using either of these methods could better represent the growth of a team over the season and how it contributes to their results over time.

Next, auxiliary, and not directly machine learning, strategies may boost log-loss performance, but may also compound the effects of incorrect predictions. 
An example of such a strategy is, if a weaker-seeded team is considered in later rounds, this means they beat stronger-seeds and therefore are a higher performing team than initially determined.
This is useful information for making predictions and could be incorporated as a feature in the model, measuring ``perceived strength'', ``updated seed'', or some other power rating. 
This also does not solely apply to weak-seeds. 
While this power rating would have a larger impact on weaker teams, it would affect any team making it further in the tournament, rewarding them for their performance.

Another such strategy could be to combine the two prediction methods above (full tournament and bracket) to generate a ``custom'' full tournament prediction. 
The bracket outlines the researchers ``selected'' predictions, based on their chosen model characteristics selected and training processes, and the full tournament can be modified to reflect this.
Each win in the bracket essentially represents a 1.0 likelihood prediction for the winning team in that game. 
To achieve maximum benefit for this potential win, the full tournament should reflect this. 
While this approach can lead to greater losses, it reflects the best estimates of the researchers selected model.

In conclusion, this research represents a solid foundation for future work in this domain. 
While many improvements can be made to the model, the results found are promising and the researchers believe incorporating the above recommendations should certainly assist in prediction accuracy. 
The 2013 tournament had the most upsets in the history of NCAA March Madness, so a bracket accuracy of approximately 50\% is an acceptable result.
Additionally, when comparing the two methods against 2014 Kaggle, it is that while the predictions do not match those of top performers (and cannot be directly compared), the best Log Loss values are in the correct range. 
The top and mean results for the 2014 season were 0.53 and 0.58 respectively, and top performing from Random Forest model was 0.61. 
%<A CONCLUDING SENTENCE>

% use section* for acknowledgement
\section*{Acknowledgment}
The authors would like to thank Ken Pomeroy for collecting, calculating, and organizing high quality NCAA basketball statistics. As well as Karl Leswing for providing access to this data. Lastly, the authors would like to thank the organizers of the Kaggle competition for inspiring this project.


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\bibliography{references}
%\begin{thebibliography}{1}

%\end{thebibliography}

\end{document}


